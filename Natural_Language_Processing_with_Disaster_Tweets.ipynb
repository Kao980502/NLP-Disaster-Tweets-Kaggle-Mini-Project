{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing with Disaster Tweets"
      ],
      "metadata": {
        "id": "w6o4B8U2hdHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Brief description of the problem and data"
      ],
      "metadata": {
        "id": "TkahdQ0mhm22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains about 7k news paragraphs that are either related or unrelated to a real disaster, labeled 1 and 0. The news paragraph may or may not come with a keyword. The problem is to predict whether a given paragraph is about a real disaster."
      ],
      "metadata": {
        "id": "LrWrasNuhuon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Exploratory Data Analysis (EDA) — Inspect, Visualize and Clean the Data"
      ],
      "metadata": {
        "id": "zs-As_39h3Yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I first checked the missing value in the given csv. I realized the location column has a lot of missing values (~1/3). On the other hand, keyword only has 23 missing values. As a result, I did not use location during training, as it complicates data processing. I also checked label distribution. It is about balanced, while negative examples are slightly more than positive ones."
      ],
      "metadata": {
        "id": "Jef4CdNriAhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture"
      ],
      "metadata": {
        "id": "AtErRojNisd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use a simple LSTM in this task. Below is the architecture.\n",
        "1.\tLSTM Layer: The core of the model, with word_vector_size as input size, hidden_size as the hidden state dimension, and n_layers stacked layers.\n",
        "2.\tFully Connected Layers:\\\n",
        "•\tfc1: Reduces the hidden state dimension to 128.\\\n",
        "•\tfc2: Further reduces the dimension to 32.\\\n",
        "•\tfc3: Outputs a single scalar for binary classification.   \n",
        "\n",
        "ReLU activation is applied after each fully connected layer, and dropout is introduced after each activation to prevent overfitting. Output Activation is Sigmoid, producing prediction result.\n"
      ],
      "metadata": {
        "id": "Aj1-pLfQixHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Analysis"
      ],
      "metadata": {
        "id": "LaSBBRegi9Nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have reached 78% acc using a simple LSTM model. While I have tested different hyperparameters, the result is about the same. The biggest difference I have made during my trials is to switch from WordtoVec to dictionary. While the latter seems to shorten runtime and increase training acc a bit, I failed to produce the testing result, as some words in the testing set is not presented in training set, and therefore not in the constructed dictionary."
      ],
      "metadata": {
        "id": "uQvOpQF2jBiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "BXmRkXmQjLVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I learned that data preprocessing can be crucial for NLP problems, unlike CV problems, where feeding pictures to CNN generally produce acceptable results"
      ],
      "metadata": {
        "id": "UoG84tjFjQWu"
      }
    }
  ]
}